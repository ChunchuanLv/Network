{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let D(Pg|Pr) be the KL Divergence between any graph (g) and a random graph (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ D(P_g|P_r) = \\sum_{d} P_g(d) \\log \\left( \\frac{P_g(d)}{ P_r(d)} \\right)$\n",
    "\n",
    "$ D(P_g|P_r) = \\sum_{d} P_g(d) \\log P_g(d) - \\sum_{d} P_g(d) \\log P_r(d) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_r(d)$ ~ Binomial distribution\n",
    "\n",
    "Hence $P_r(d) = \\left( _d ^{n-1} \\right) p^d (1-p)^{n-1-d}$\n",
    "\n",
    "$\\frac{\\partial D}{\\partial P_r} = 0 - \\frac{\\partial (\\sum_{d} P_g(d) . log(P_r(d)) )}{\\partial P_r} $\n",
    "\n",
    "Minimizing $D$\n",
    "\n",
    "$\\frac{\\partial D}{\\partial p} = \\frac{\\partial D}{\\partial P_r} \\frac{\\partial P_r}{\\partial p} =0 $\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$ - \\frac{\\partial }{\\partial p} \\sum_{d} {P_g(d)[ d  \\log(p) + (n-1-d)\\log(1-p)]} = 0$\n",
    "\n",
    "$ -\\sum_{d} {P_g(d)[ \\frac{d}{p} - \\frac {(n-1-d)}{(1-p)}} ] = 0 $\n",
    "\n",
    "$ -\\sum_{d} P_g(d) . \\frac{d}{p} - \\sum_{d} P_g(d) \\frac {(n-1-d)}{(1-p)}  = 0 $   --> (1)\n",
    "\n",
    "But, the expected value of d in g is given as: \n",
    "\n",
    "$ E_g[d] = \\sum_{d} P_g(d) . d $\n",
    "\n",
    "So (1) becomes,\n",
    "\n",
    "$\\frac{n-1}{1-p} - \\frac{E_g[d]}{1-p} = \\frac{E_g[d]}{p} $\n",
    "\n",
    "$(n-1) p - E_g[d] p = E_g[d](1-p) $\n",
    "\n",
    "$\\frac{E_g[d]}{p} = (n-1)p $\n",
    "\n",
    "$E_g[d] = \\frac{d_{total}}{n}$\n",
    "\n",
    "So,\n",
    "\n",
    "$p = \\frac{d_{total}}{n(n-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_r(d)$ ~ Binomial distribution\n",
    "\n",
    "Hence $P_r(d) = \\left( _d ^{n-1} \\right) p^d (1-p)^{n-1-d}$\n",
    "\n",
    "$\\frac{\\partial D}{\\partial P_r} = 0 - \\frac{\\partial (\\sum_{d} P_g(d) . log(P_r(d)) )}{\\partial P_r} $\n",
    "\n",
    "Maximising $\\frac{\\partial D}{\\partial P_r} $\n",
    "\n",
    "$\\frac{\\partial D}{\\partial P_r} = 0 $\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$ - \\frac{\\partial }{\\partial P_r} \\sum_{d} {P_g(d)[ d  \\log(P_r(d)) + (n-1-d)\\log(1-P_r)]} = 0$\n",
    "\n",
    "$ -\\sum_{d} {P_g(d)[ \\frac{d}{P_r} - \\frac {(n-1-d)}{(1-P_r)}} ] = 0 $\n",
    "\n",
    "$ -\\sum_{d} P_g(d) . \\frac{d}{P_r} - \\sum_{d} P_g(d) \\frac {(n-1-d)}{(1-P_r)}  = 0 $   --> (1)\n",
    "\n",
    "But, the expected value of d in g is given as: \n",
    "\n",
    "$ E_g[d] = \\sum_{d} P_g(d) . d $\n",
    "\n",
    "So (1) becomes,\n",
    "\n",
    "$\\frac{n-1}{1-P_r} - \\frac{E[d]}{1-P_r} = \\frac{E_g[d]}{P_r} $\n",
    "\n",
    "$\\frac{E_g[d]}{P_r} = (n-1)P_r $\n",
    "\n",
    "$E_g[d] = \\frac{d_{total}}{n}$\n",
    "\n",
    "So,\n",
    "\n",
    "$P_r = \\frac{d_{total}}{n(n-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
